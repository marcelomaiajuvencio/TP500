{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Estudo de DDPG com Pendulum-v1 do Gym/Gymnasium e PyTorch\n",
        "\n",
        "O `Pendulum-v1` é um **ambiente de simulação clássico** de aprendizado por reforço (Reinforcement Learning), incluído na biblioteca **Gym** (ou **Gymnasium**, sua versão mais atualizada).\n",
        "\n",
        "Ele simula um **pêndulo invertido** preso por uma junta sem atrito, e o objetivo é **mantê-lo em pé (posição vertical)** aplicando torques na base.\n",
        "\n",
        "---  \n",
        "\n",
        "### Outros ambientes\n",
        "\n",
        "| Ambiente                | Ação         | Complexidade | Observação                            |\n",
        "|-------------------------|--------------|--------------|----------------------------------------|\n",
        "| Pendulum-v1             | 1D contínua  | Baixa        | Simples, ótimo para DDPG inicial       |\n",
        "| MountainCarContinuous-v0| 1D contínua  | Média        | Exploração importante                  |\n",
        "| Reacher-v2              | 2D contínua  | Média        | Braço robótico                         |\n",
        "| LunarLanderContinuous-v2| 2D contínua  | Média        | Controle com física realista           |\n",
        "| HalfCheetah-v2          | N-dim contínua| Alta        | Benchmark clássico para RL contínuo    |\n",
        "| Hopper-v2               | N-dim contínua| Alta        | Controle de salto com estabilidade     |\n",
        "| Walker2d-v2             | N-dim contínua| Muito Alta  | Locomoção bípede                       |\n",
        "| Ant-v2                  | N-dim contínua| Muito Alta  | Locomoção em 4 patas                   |\n",
        "| Humanoid-v2             | N-dim contínua| Muito Alta  | Controle complexo de corpo humanoide   |  \n",
        "\n",
        "---\n",
        "\n",
        "### Variações e Extensões do DDPG\n",
        "\n",
        "| Variação / Extensão     | Descrição breve                                                                 |\n",
        "|--------------------------|---------------------------------------------------------------------------------|\n",
        "| **TD3**                 | Usa dois críticos e atraso na atualização do ator para maior estabilidade.     |\n",
        "| **SAC**                 | Algoritmo off-policy com entropia máxima para melhor exploração.                |\n",
        "| **HER + DDPG**          | Usa experiências com metas alternativas em tarefas com sparse reward.          |\n",
        "| **PER + DDPG**          | Priorização de experiências mais informativas na amostragem do replay buffer.  |\n",
        "| **MADDPG**              | Extensão multiagente para ambientes com múltiplos agentes interativos.         |\n",
        "| **Distributional DDPG**| Modela a distribuição do retorno em vez do valor esperado, melhorando precisão. |\n",
        "| **Meta-DDPG / MAML-DDPG** | Aplica aprendizado de meta-política para adaptação rápida a novas tarefas.    |\n",
        "| **Noisy DDPG**          | Introduz ruído nos pesos da rede para promover exploração eficiente.            |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "F8qV8ry86RZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn # Ver stable_baselines3 (com \"MlpPolicy\") parece mais simples\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# Configuração de dispositivo (GPU via cuda(Compute Unified Device Architecture) ou CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Replay Buffer - armazena interações do agente com o ambiente\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=100000):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        samples = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*samples)\n",
        "        return map(lambda x: torch.FloatTensor(x).to(device), [states, actions, rewards, next_states, dones])\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# Redes\n",
        "# Ator (Actor): rede neural (nn) que decide ações dado o estado, retorna action (tensor) a = μ(s)\n",
        "# Funções de ativação ReLU (Rectified Linear Unit) e tanh (Tangente Hiperbólica)\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, action_bound):\n",
        "        super(Actor, self).__init__()\n",
        "        self.action_bound = action_bound\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 256), nn.ReLU(),\n",
        "            nn.Linear(256, action_dim), nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.model(state) * self.action_bound\n",
        "\n",
        "\n",
        "# Crítico (Critic): rede neural (nn) que avalia a qualidade da ação, retorna Q-value (escalar) Q(s, a)\n",
        "# Funçoes de ativação ReLU (Rectified Linear Unit)\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        return self.model(torch.cat([state, action], 1))\n",
        "\n",
        "# Agente\n",
        "class DDPGAgent:\n",
        "    def __init__(self, state_dim, action_dim, action_bound):\n",
        "        self.actor = Actor(state_dim, action_dim, action_bound).to(device)\n",
        "        self.target_actor = Actor(state_dim, action_dim, action_bound).to(device)\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.target_critic = Critic(state_dim, action_dim).to(device)\n",
        "\n",
        "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
        "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "        # Atualiza pesos com Adam (Otimizador) e respectivos lr (learning rate - taxa de aprendizado)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "\n",
        "        self.buffer = ReplayBuffer()\n",
        "        self.gamma = 0.99\n",
        "        # taxa de atualização suave\n",
        "        self.tau = 0.005\n",
        "        self.action_bound = action_bound\n",
        "\n",
        "    def act(self, state, noise=0.1):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        action = self.actor(state).detach().cpu().numpy()[0]\n",
        "        action += noise * np.random.randn(*action.shape)\n",
        "        return np.clip(action, -self.action_bound, self.action_bound)\n",
        "\n",
        "    def update(self, batch_size=64):\n",
        "        if self.buffer.size() < batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.buffer.sample(batch_size)\n",
        "\n",
        "        # Atualiza crítico\n",
        "        with torch.no_grad():\n",
        "            target_actions = self.target_actor(next_states)\n",
        "            target_q = self.target_critic(next_states, target_actions)\n",
        "            y = rewards.unsqueeze(1) + self.gamma * (1 - dones.unsqueeze(1)) * target_q\n",
        "\n",
        "        q = self.critic(states, actions)\n",
        "        critic_loss = nn.MSELoss()(q, y)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Atualiza ator\n",
        "        actor_loss = -self.critic(states, self.actor(states)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Atualização suave dos alvos (soft update)\n",
        "        # Actor\n",
        "        for param, target_param in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "        # Critic\n",
        "        for param, target_param in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "# Treinamento\n",
        "env = gym.make(\"Pendulum-v1\")\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "action_bound = float(env.action_space.high[0])\n",
        "\n",
        "agent = DDPGAgent(state_dim, action_dim, action_bound)\n",
        "# Inicialize antes do loop de episódios\n",
        "rewards = []\n",
        "episodes = 100\n",
        "for ep in range(episodes):\n",
        "    state, _ = env.reset()\n",
        "    episode_reward = 0\n",
        "    for step in range(200):\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        agent.buffer.add((state, action, reward, next_state, float(done)))\n",
        "        agent.update()\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    rewards.append(episode_reward)  # <- Aqui adicionamos a recompensa final do episódio\n",
        "    print(f\"Episode {ep + 1}: Reward = {episode_reward:.2f}\")\n",
        "\n",
        "    torch.save(agent.actor.state_dict(), 'ddpg_actor.pth')\n",
        "    torch.save(agent.critic.state_dict(), 'ddpg_critic.pth')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MK-R62Oz8P9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráfico com e sem suavização\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def moving_average(data, window_size=10):\n",
        "    return [sum(data[i-window_size:i])/window_size if i >= window_size else data[i] for i in range(len(data))]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(rewards, label='Recompensa (original)', alpha=0.3)\n",
        "plt.plot(moving_average(rewards, window_size=10), label='Média móvel (janela=10)', color='orange', linewidth=2)\n",
        "plt.xlabel(\"Episódio\")\n",
        "plt.ylabel(\"Recompensa total\")\n",
        "plt.title(\"Desempenho do agente DDPG no Pendulum\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "# Salvar o gráfico como imagem PNG\n",
        "plt.savefig(f\"recompensas_ddpg_{episodes}.png\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "vo2Jir1gCtEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar os pesos e gerar um vídeo Versão2\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Corrigir tipo bool8 se necessário\n",
        "if not hasattr(np, 'bool8'):\n",
        "    np.bool8 = np.bool_\n",
        "\n",
        "# Criar o ambiente com modo de renderização\n",
        "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "action_bound = env.action_space.high[0]\n",
        "\n",
        "# Redes\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, action_bound):\n",
        "        super(Actor, self).__init__()\n",
        "        self.action_bound = action_bound\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 256), nn.ReLU(),\n",
        "            nn.Linear(256, action_dim), nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.model(state) * self.action_bound\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        return self.model(torch.cat([state, action], 1))\n",
        "\n",
        "\n",
        "class DDPGAgent:\n",
        "    def __init__(self, state_dim, action_dim, action_bound):\n",
        "        self.actor = Actor(state_dim, action_dim, action_bound)\n",
        "        self.critic = Critic(state_dim, action_dim)\n",
        "\n",
        "agent = DDPGAgent(state_dim, action_dim, action_bound)\n",
        "agent.actor.load_state_dict(torch.load(\"ddpg_actor.pth\"))\n",
        "agent.critic.load_state_dict(torch.load(\"ddpg_critic.pth\"))\n",
        "\n",
        "\n",
        "def generate_video():\n",
        "    frames = []\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        action = float(agent.actor(state_tensor).detach().numpy()[0])\n",
        "        next_state, reward, terminated, truncated, _ = env.step(np.array([action]))\n",
        "        done = terminated or truncated\n",
        "\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        state = next_state\n",
        "\n",
        "    height, width, _ = frames[0].shape\n",
        "    writer = cv2.VideoWriter(f\"pendulum_video_{episodes}.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), 30, (width, height))\n",
        "\n",
        "    for frame in frames:\n",
        "        # frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "        # writer.write(frame_bgr)\n",
        "        writer.write(frame)\n",
        "    writer.release()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "generate_video()\n"
      ],
      "metadata": {
        "id": "yRtrqAdlXQ0H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}